{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XMTZWzJ3dOif"
   },
   "source": [
    "# Gospel of LSTMs\n",
    "\n",
    "- This notebook train, evaluate and generate Gospels of Bible. The code is originally forked from [Official Word Level Language Modelling example of PyTorch](https://github.com/pytorch/examples/tree/master/word_language_model)  and altered in accordance with the particular use case. \n",
    "\n",
    "- The cleaned Gospel data is available at [here](https://github.com/sleebapaul/gospel_of_rnn/tree/master/training_data). \n",
    "\n",
    "- Used PyTorch version is  0.4.0. \n",
    "\n",
    "- If you're a beginner to RNNs, you may find the comments really helpful. So I commented a lot in this notebook. \n",
    "\n",
    "- If you find a difficulty in reproducing the results locally using this Jupyter Notebook, well I would say that's expected. Especially, the data interaction part is needed to be rewritten and it is an additional effort. Because, this notebook is originally a Colab Notebook and architectured in favour of same.  So I recommend the reader to consider this one a local reference and go to [the original Colab notebook](https://colab.research.google.com/drive/1euakjbNiZgCfbmCWzT6pIZB2MYZbHjk-) for a better life!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "colab_type": "code",
    "id": "OxkTuROKdJ8f",
    "outputId": "15cb058e-49f1-43dc-c061-c07980d1f1d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 1073750016 bytes == 0x5c996000 @  0x7fb0414791c4 0x46d6a4 0x5fcbcc 0x4c494d 0x54f3c4 0x553aaf 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54e4c8\n",
      "Collecting pydrive\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e0/0e64788e5dd58ce2d6934549676243dc69d982f198524be9b99e9c2a4fd5/PyDrive-1.3.1.tar.gz (987kB)\n",
      "\u001b[K    100% |████████████████████████████████| 993kB 5.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from pydrive) (1.6.7)\n",
      "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (4.1.2)\n",
      "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (3.13)\n",
      "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (1.11.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (3.0.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (0.11.3)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.4.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.2.2)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (3.4.2)\n",
      "Building wheels for collected packages: pydrive\n",
      "  Running setup.py bdist_wheel for pydrive ... \u001b[?25l-\b \b\\\b \bdone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/fa/d2/9a/d3b6b506c2da98289e5d417215ce34b696db856643bad779f4\n",
      "Successfully built pydrive\n",
      "Installing collected packages: pydrive\n",
      "Successfully installed pydrive-1.3.1\n"
     ]
    }
   ],
   "source": [
    "# Download and setup PyTorch\n",
    "\n",
    "from os import path\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "\n",
    "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
    "\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision\n",
    "    \n",
    "!pip install pydrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "SkI-Q5c_9kGG",
    "outputId": "848e8529-84a7-4b33-d9f4-b9e018720e95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  0.4.0\n",
      "Cuda availability:  True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f928cf03fb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Cuda availability: \",torch.cuda.is_available())\n",
    "\n",
    "# for reproducability set the seed \n",
    "\n",
    "torch.manual_seed(1111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2zQKvkhn9tlX"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "from google.colab import files\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "v0riS4Nc9zst",
    "outputId": "67b7929f-cd1d-4f34-bbc6-894e3e324533"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gSxccmTziBIL"
   },
   "source": [
    "# Helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g-4s8n6W94tF"
   },
   "outputs": [],
   "source": [
    "def get_text_files(folder_name):\n",
    "    \"\"\"\n",
    "    Get text files from uploaded files\n",
    "    \"\"\"\n",
    "    text_files = []\n",
    "    for file in os.listdir(\".\"):\n",
    "        if file.endswith(\".txt\"):\n",
    "            text_files.append(file)\n",
    "    return sorted(text_files)\n",
    "\n",
    "\n",
    "def store_files(file_list):\n",
    "    \"\"\"\n",
    "    Read and store files in the file_list as strings \n",
    "    Returns a dictionary with keys as file names and values as their contents\n",
    "    \"\"\"\n",
    "    output = {}\n",
    "    for file in file_list:\n",
    "        with open(file, 'r') as f:\n",
    "            output[file] = f.read()\n",
    "    return output\n",
    "        \n",
    "\n",
    "def dictionary(training_data):\n",
    "    \"\"\"\n",
    "    Create word2idx and idx2word dictionaries \n",
    "    Returns two dictionaries\n",
    "    \"\"\"\n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    words = training_data.split()\n",
    "    token = 0\n",
    "    for word in words: \n",
    "        if not word in word2idx:\n",
    "            word2idx[word] = token\n",
    "            idx2word[token] = word\n",
    "            token +=1\n",
    "    return word2idx, idx2word\n",
    "\n",
    "\n",
    "def tensor_generator(training_data, word2idx):\n",
    "    \"\"\"\n",
    "    Create a PyTorch LongTensor for the training data\n",
    "    Returns a 1D tensor with ids of words  \n",
    "    \"\"\"\n",
    "    words = training_data.split()\n",
    "    ids = torch.LongTensor(len(words))\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        ids[i] = word2idx[word]\n",
    "        \n",
    "    return ids\n",
    "\n",
    "        \n",
    "def batchify(data, bsz):\n",
    "    \"\"\"\n",
    "    Batch the data with size bsz\n",
    "    \"\"\"\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "\n",
    "def get_batch(source, i):\n",
    "    \"\"\"\n",
    "    Select seq_length long batches at once for training\n",
    "    Data and Targets will be differed in index by one\n",
    "    \"\"\"\n",
    "    seq_len = min(seq_length, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target\n",
    "\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"\n",
    "    Wraps hidden states in new Tensors, to detach them from their history.\n",
    "    For truncated backpropagation \n",
    "    \"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Container module with an encoder, a recurrent module, and a decoder.\n",
    "    PyTorch provides the facility to write custom models\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "\n",
    "        self.LSTM = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.LSTM(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "\n",
    "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
    "                weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
    "    \n",
    "\n",
    "def evaluate(data_source, eval_batch_size):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of the trained model in input data source\n",
    "    \"\"\"\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    ntokens = vocab_size\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, seq_length):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output, hidden = model(data, hidden)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "            hidden = repackage_hidden(hidden)\n",
    "    return total_loss / len(data_source)\n",
    "\n",
    "\n",
    "def train():\n",
    "    \"\"\"\n",
    "    Training - Full Throttle :D \n",
    "    \"\"\"\n",
    "    # Turn on training mode which enables dropout.\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = vocab_size\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, seq_length)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        \n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        \n",
    "        hidden = repackage_hidden(hidden)\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        clip_grad_norm_(model.parameters(), 0.25)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:5.4f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f} |'.format(\n",
    "                epoch, batch, len(train_data) // seq_length, learning_rate,\n",
    "                elapsed * 1000 /log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "            \n",
    "            \n",
    "def get_warmup_state(data_source):\n",
    "    \"\"\"\n",
    "    Starting hidden states as zeros might not deliver the context \n",
    "    So a warm up is on a desired primer text \n",
    "    Returns the hidden state for actual generation \n",
    "    \"\"\"\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    ntokens = vocab_size\n",
    "    hidden = model.init_hidden(1)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, seq_length):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            output, hidden = model(data, hidden)         \n",
    "    return hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DXPte_Pfh5M6"
   },
   "source": [
    "# Data handling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1276,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "Tlfe-cxK-BmO",
    "outputId": "6fefaffb-852b-457a-c99a-ed903c35b095"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '*.txt': No such file or directory\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-e339e63f-ab8d-4066-8509-e208cb6f7d0b\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-e339e63f-ab8d-4066-8509-e208cb6f7d0b\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving john_asv.txt to john_asv.txt\n",
      "Saving john_bbe.txt to john_bbe.txt\n",
      "Saving john_dby.txt to john_dby.txt\n",
      "Saving john_kjv.txt to john_kjv.txt\n",
      "Saving john_wbt.txt to john_wbt.txt\n",
      "Saving john_ylt.txt to john_ylt.txt\n",
      "Saving luke_asv.txt to luke_asv.txt\n",
      "Saving luke_bbe.txt to luke_bbe.txt\n",
      "Saving luke_dby.txt to luke_dby.txt\n",
      "Saving luke_kjv.txt to luke_kjv.txt\n",
      "Saving luke_wbt.txt to luke_wbt.txt\n",
      "Saving luke_ylt.txt to luke_ylt.txt\n",
      "Saving mark_asv.txt to mark_asv.txt\n",
      "Saving mark_bbe.txt to mark_bbe.txt\n",
      "Saving mark_dby.txt to mark_dby.txt\n",
      "Saving mark_kjv.txt to mark_kjv.txt\n",
      "Saving mark_wbt.txt to mark_wbt.txt\n",
      "Saving mark_ylt.txt to mark_ylt.txt\n",
      "Saving matthew_asv.txt to matthew_asv.txt\n",
      "Saving matthew_bbe.txt to matthew_bbe.txt\n",
      "Saving matthew_dby.txt to matthew_dby.txt\n",
      "Saving matthew_kjv.txt to matthew_kjv.txt\n",
      "Saving matthew_wbt.txt to matthew_wbt.txt\n",
      "Saving matthew_ylt.txt to matthew_ylt.txt\n",
      "User uploaded file \"john_asv.txt\" with length 112922 bytes\n",
      "User uploaded file \"john_bbe.txt\" with length 116770 bytes\n",
      "User uploaded file \"john_dby.txt\" with length 112895 bytes\n",
      "User uploaded file \"john_kjv.txt\" with length 112287 bytes\n",
      "User uploaded file \"john_wbt.txt\" with length 111436 bytes\n",
      "User uploaded file \"john_ylt.txt\" with length 119408 bytes\n",
      "User uploaded file \"luke_asv.txt\" with length 152492 bytes\n",
      "User uploaded file \"luke_bbe.txt\" with length 155973 bytes\n",
      "User uploaded file \"luke_dby.txt\" with length 151995 bytes\n",
      "User uploaded file \"luke_kjv.txt\" with length 153228 bytes\n",
      "User uploaded file \"luke_wbt.txt\" with length 152164 bytes\n",
      "User uploaded file \"luke_ylt.txt\" with length 158139 bytes\n",
      "User uploaded file \"mark_asv.txt\" with length 89364 bytes\n",
      "User uploaded file \"mark_bbe.txt\" with length 91145 bytes\n",
      "User uploaded file \"mark_dby.txt\" with length 89983 bytes\n",
      "User uploaded file \"mark_kjv.txt\" with length 90092 bytes\n",
      "User uploaded file \"mark_wbt.txt\" with length 89534 bytes\n",
      "User uploaded file \"mark_ylt.txt\" with length 93369 bytes\n",
      "User uploaded file \"matthew_asv.txt\" with length 140585 bytes\n",
      "User uploaded file \"matthew_bbe.txt\" with length 143218 bytes\n",
      "User uploaded file \"matthew_dby.txt\" with length 140867 bytes\n",
      "User uploaded file \"matthew_kjv.txt\" with length 141711 bytes\n",
      "User uploaded file \"matthew_wbt.txt\" with length 140534 bytes\n",
      "User uploaded file \"matthew_ylt.txt\" with length 145176 bytes\n"
     ]
    }
   ],
   "source": [
    "#Once run this will allow you to manually select the path on local drive for file you wish to upload\n",
    "!rm *.txt\n",
    "uploaded = files.upload()\n",
    "for file in uploaded.keys():\n",
    "    print('User uploaded file \"{name}\" with length {length} bytes'.format(name=file, length=len(uploaded[file])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "HoOuEuEf-BpH",
    "outputId": "642a215d-9ac9-4208-d460-615214aebf4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'generated_sample.txt': No such file or directory\n",
      "Files uploaded:  john_asv.txt, john_bbe.txt, john_dby.txt, john_kjv.txt, john_wbt.txt, john_ylt.txt, luke_asv.txt, luke_bbe.txt, luke_dby.txt, luke_kjv.txt, luke_wbt.txt, luke_ylt.txt, mark_asv.txt, mark_bbe.txt, mark_dby.txt, mark_kjv.txt, mark_wbt.txt, mark_ylt.txt, matthew_asv.txt, matthew_bbe.txt, matthew_dby.txt, matthew_kjv.txt, matthew_wbt.txt, matthew_ylt.txt\n",
      "Number of files:  24\n"
     ]
    }
   ],
   "source": [
    "! rm generated_sample.txt\n",
    "\n",
    "txt_files = get_text_files(\".\")\n",
    "print(\"Files uploaded: \", \", \".join(txt_files))\n",
    "print(\"Number of files: \", len(txt_files))\n",
    "\n",
    "corpora = store_files(txt_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "2bzt1UHI_BV_",
    "outputId": "c22e143d-4abf-451e-edc5-06718c0c7ee7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training files (22): john_asv.txt, john_bbe.txt, john_dby.txt, john_kjv.txt, john_wbt.txt, john_ylt.txt, luke_asv.txt, luke_bbe.txt, luke_dby.txt, luke_kjv.txt, luke_wbt.txt, luke_ylt.txt, mark_bbe.txt, mark_dby.txt, mark_wbt.txt, mark_ylt.txt, matthew_asv.txt, matthew_bbe.txt, matthew_dby.txt, matthew_kjv.txt, matthew_wbt.txt, matthew_ylt.txt\n",
      "Validation file :  mark_asv.txt\n",
      "Testing files :  mark_kjv.txt\n"
     ]
    }
   ],
   "source": [
    "# Copy complete corpora to training\n",
    "training_files = list(corpora.keys())\n",
    "\n",
    "# Select a file for validation and remove it from training data\n",
    "val_file = random.choice(training_files)\n",
    "training_files.pop(training_files.index(val_file))\n",
    "\n",
    "# Select a file for test and remove it from training data\n",
    "test_file = random.choice(training_files)\n",
    "training_files.pop(training_files.index(test_file))\n",
    "\n",
    "print(\"Training files ({}): {}\".format(len(training_files), \", \".join(training_files)))\n",
    "print(\"Validation file : \", val_file)\n",
    "print(\"Testing files : \", test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "GZUZmpsS-J3S",
    "outputId": "4688dd89-eeb6-4d08-88ef-cf31a462471a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in entire corpus:  653611\n",
      "Vocabulary size:  6067\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the vocabulary by combining all books\n",
    "\n",
    "combined_corpus = \" \".join(list(corpora.values()))\n",
    "\n",
    "word2idx, idx2word = dictionary(combined_corpus)\n",
    "\n",
    "print(\"Total number of words in entire corpus: \", len(combined_corpus.split()))\n",
    "\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "print(\"Vocabulary size: \", vocab_size)\n",
    "\n",
    "del combined_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "izB-7dgd-M4-",
    "outputId": "be775bdd-dbc6-4498-ece3-db4353073a0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of Jesus:  120\n",
      "Word of index 120:  Jesus\n"
     ]
    }
   ],
   "source": [
    "print(\"Index of Jesus: \", word2idx[\"Jesus\"])\n",
    "print(\"Word of index {}:  {}\".format(word2idx[\"Jesus\"], idx2word[word2idx[\"Jesus\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HmpO2RS-_29O"
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "\n",
    "embed_size = 300\n",
    "hidden_size = 1024\n",
    "num_layers = 2\n",
    "num_epochs = 100\n",
    "batch_size = 30\n",
    "seq_length = 35\n",
    "learning_rate = 20.0\n",
    "dropout_value = 0.4\n",
    "log_interval = 100\n",
    "eval_batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EybjowFuzfGI"
   },
   "outputs": [],
   "source": [
    "# Concatenate the training data into a single corpus by selecting corresponding files from corpora\n",
    "training_data = \"\"\n",
    "for file in training_files:\n",
    "    training_data += corpora[file] + \" \"\n",
    "training_data = training_data.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HtNKmNpb-BsP"
   },
   "outputs": [],
   "source": [
    "# Batchify every data set\n",
    "train_data = batchify(tensor_generator(training_data, word2idx), batch_size)\n",
    "val_data = batchify(tensor_generator(corpora[val_file], word2idx), eval_batch_size)\n",
    "test_data = batchify(tensor_generator(corpora[test_file],word2idx), eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "z9VDliLVgs04",
    "outputId": "cf7b2846-f5f9-459f-e0f4-a67b88790c24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of batchified training data:  torch.Size([20509, 30])\n",
      "Shape of batchified validation data:  torch.Size([1911, 10])\n",
      "Shape of batchified testing data:  torch.Size([1919, 10])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of batchified training data: \", train_data.shape)\n",
    "print(\"Shape of batchified validation data: \", val_data.shape)\n",
    "print(\"Shape of batchified testing data: \", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Fuoq6YjiEtX"
   },
   "source": [
    "# Model definition and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PJ0qB6wuBSO3"
   },
   "outputs": [],
   "source": [
    "# Define model for training \n",
    "\n",
    "model = RNNModel(ntoken=vocab_size, ninp=embed_size, nhid=hidden_size, nlayers=num_layers, dropout=dropout_value).to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 9571
    },
    "colab_type": "code",
    "id": "zJxtqp9bClkC",
    "outputId": "1d6c8e9c-9d08-4853-e6c2-f2de24af4f42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   100/  562 batches | lr 20.0000 | ms/batch 117.56 | loss  6.61 | ppl   741.10 |\n",
      "| epoch   1 |   200/  562 batches | lr 20.0000 | ms/batch 113.70 | loss  5.09 | ppl   161.68 |\n",
      "| epoch   1 |   300/  562 batches | lr 20.0000 | ms/batch 113.86 | loss  4.68 | ppl   107.50 |\n",
      "| epoch   1 |   400/  562 batches | lr 20.0000 | ms/batch 113.94 | loss  4.37 | ppl    79.19 |\n",
      "| epoch   1 |   500/  562 batches | lr 20.0000 | ms/batch 113.93 | loss  4.14 | ppl    62.94 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 90.22s | valid loss  4.01 | valid ppl    55.36 | training loss  3.97 | training ppl    53.17 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch   2 |   100/  562 batches | lr 20.0000 | ms/batch 115.20 | loss  3.96 | ppl    52.25 |\n",
      "| epoch   2 |   200/  562 batches | lr 20.0000 | ms/batch 114.50 | loss  3.83 | ppl    45.85 |\n",
      "| epoch   2 |   300/  562 batches | lr 20.0000 | ms/batch 114.83 | loss  3.77 | ppl    43.45 |\n",
      "| epoch   2 |   400/  562 batches | lr 20.0000 | ms/batch 114.79 | loss  3.63 | ppl    37.75 |\n",
      "| epoch   2 |   500/  562 batches | lr 20.0000 | ms/batch 114.47 | loss  3.51 | ppl    33.38 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 90.36s | valid loss  3.40 | valid ppl    29.96 | training loss  3.30 | training ppl    27.16 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch   3 |   100/  562 batches | lr 20.0000 | ms/batch 115.96 | loss  3.43 | ppl    30.93 |\n",
      "| epoch   3 |   200/  562 batches | lr 20.0000 | ms/batch 114.69 | loss  3.33 | ppl    27.84 |\n",
      "| epoch   3 |   300/  562 batches | lr 20.0000 | ms/batch 114.68 | loss  3.32 | ppl    27.60 |\n",
      "| epoch   3 |   400/  562 batches | lr 20.0000 | ms/batch 114.69 | loss  3.20 | ppl    24.45 |\n",
      "| epoch   3 |   500/  562 batches | lr 20.0000 | ms/batch 114.73 | loss  3.09 | ppl    21.89 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 90.51s | valid loss  2.91 | valid ppl    18.42 | training loss  2.81 | training ppl    16.56 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch   4 |   100/  562 batches | lr 20.0000 | ms/batch 115.89 | loss  3.03 | ppl    20.71 |\n",
      "| epoch   4 |   200/  562 batches | lr 20.0000 | ms/batch 114.74 | loss  2.95 | ppl    19.09 |\n",
      "| epoch   4 |   300/  562 batches | lr 20.0000 | ms/batch 114.85 | loss  2.95 | ppl    19.17 |\n",
      "| epoch   4 |   400/  562 batches | lr 20.0000 | ms/batch 114.94 | loss  2.84 | ppl    17.06 |\n",
      "| epoch   4 |   500/  562 batches | lr 20.0000 | ms/batch 114.80 | loss  2.74 | ppl    15.44 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 90.54s | valid loss  2.48 | valid ppl    11.97 | training loss  2.37 | training ppl    10.64 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch   5 |   100/  562 batches | lr 20.0000 | ms/batch 116.02 | loss  2.68 | ppl    14.59 |\n",
      "| epoch   5 |   200/  562 batches | lr 20.0000 | ms/batch 114.87 | loss  2.62 | ppl    13.67 |\n",
      "| epoch   5 |   300/  562 batches | lr 20.0000 | ms/batch 114.95 | loss  2.63 | ppl    13.87 |\n",
      "| epoch   5 |   400/  562 batches | lr 20.0000 | ms/batch 114.79 | loss  2.53 | ppl    12.52 |\n",
      "| epoch   5 |   500/  562 batches | lr 20.0000 | ms/batch 114.69 | loss  2.42 | ppl    11.28 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 90.56s | valid loss  2.11 | valid ppl     8.26 | training loss  2.01 | training ppl     7.45 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch   6 |   100/  562 batches | lr 20.0000 | ms/batch 115.68 | loss  2.38 | ppl    10.81 |\n",
      "| epoch   6 |   200/  562 batches | lr 20.0000 | ms/batch 114.95 | loss  2.34 | ppl    10.36 |\n",
      "| epoch   6 |   300/  562 batches | lr 20.0000 | ms/batch 115.10 | loss  2.36 | ppl    10.57 |\n",
      "| epoch   6 |   400/  562 batches | lr 20.0000 | ms/batch 115.26 | loss  2.26 | ppl     9.60 |\n",
      "| epoch   6 |   500/  562 batches | lr 20.0000 | ms/batch 115.26 | loss  2.17 | ppl     8.79 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 90.69s | valid loss  1.79 | valid ppl     5.96 | training loss  1.71 | training ppl     5.52 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch   7 |   100/  562 batches | lr 20.0000 | ms/batch 115.82 | loss  2.14 | ppl     8.52 |\n",
      "| epoch   7 |   200/  562 batches | lr 20.0000 | ms/batch 114.54 | loss  2.11 | ppl     8.22 |\n",
      "| epoch   7 |   300/  562 batches | lr 20.0000 | ms/batch 114.59 | loss  2.14 | ppl     8.52 |\n",
      "| epoch   7 |   400/  562 batches | lr 20.0000 | ms/batch 114.36 | loss  2.05 | ppl     7.76 |\n",
      "| epoch   7 |   500/  562 batches | lr 20.0000 | ms/batch 114.63 | loss  1.97 | ppl     7.18 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 90.39s | valid loss  1.58 | valid ppl     4.88 | training loss  1.48 | training ppl     4.38 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch   8 |   100/  562 batches | lr 20.0000 | ms/batch 116.30 | loss  1.94 | ppl     6.96 |\n",
      "| epoch   8 |   200/  562 batches | lr 20.0000 | ms/batch 115.08 | loss  1.92 | ppl     6.80 |\n",
      "| epoch   8 |   300/  562 batches | lr 20.0000 | ms/batch 115.08 | loss  1.96 | ppl     7.07 |\n",
      "| epoch   8 |   400/  562 batches | lr 20.0000 | ms/batch 115.05 | loss  1.88 | ppl     6.53 |\n",
      "| epoch   8 |   500/  562 batches | lr 20.0000 | ms/batch 115.06 | loss  1.81 | ppl     6.08 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 90.67s | valid loss  1.38 | valid ppl     3.98 | training loss  1.29 | training ppl     3.64 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch   9 |   100/  562 batches | lr 20.0000 | ms/batch 116.15 | loss  1.78 | ppl     5.92 |\n",
      "| epoch   9 |   200/  562 batches | lr 20.0000 | ms/batch 115.19 | loss  1.76 | ppl     5.83 |\n",
      "| epoch   9 |   300/  562 batches | lr 20.0000 | ms/batch 114.91 | loss  1.81 | ppl     6.09 |\n",
      "| epoch   9 |   400/  562 batches | lr 20.0000 | ms/batch 114.67 | loss  1.73 | ppl     5.63 |\n",
      "| epoch   9 |   500/  562 batches | lr 20.0000 | ms/batch 115.19 | loss  1.67 | ppl     5.29 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 91.41s | valid loss  1.24 | valid ppl     3.46 | training loss  1.14 | training ppl     3.14 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  10 |   100/  562 batches | lr 20.0000 | ms/batch 118.13 | loss  1.65 | ppl     5.18 |\n",
      "| epoch  10 |   200/  562 batches | lr 20.0000 | ms/batch 117.02 | loss  1.64 | ppl     5.16 |\n",
      "| epoch  10 |   300/  562 batches | lr 20.0000 | ms/batch 116.91 | loss  1.69 | ppl     5.40 |\n",
      "| epoch  10 |   400/  562 batches | lr 20.0000 | ms/batch 116.96 | loss  1.62 | ppl     5.03 |\n",
      "| epoch  10 |   500/  562 batches | lr 20.0000 | ms/batch 116.84 | loss  1.56 | ppl     4.77 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 92.61s | valid loss  1.12 | valid ppl     3.08 | training loss  1.03 | training ppl     2.80 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  11 |   100/  562 batches | lr 20.0000 | ms/batch 117.66 | loss  1.54 | ppl     4.66 |\n",
      "| epoch  11 |   200/  562 batches | lr 20.0000 | ms/batch 116.59 | loss  1.53 | ppl     4.63 |\n",
      "| epoch  11 |   300/  562 batches | lr 20.0000 | ms/batch 116.24 | loss  1.59 | ppl     4.89 |\n",
      "| epoch  11 |   400/  562 batches | lr 20.0000 | ms/batch 116.56 | loss  1.52 | ppl     4.56 |\n",
      "| epoch  11 |   500/  562 batches | lr 20.0000 | ms/batch 116.07 | loss  1.47 | ppl     4.34 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 92.23s | valid loss  1.03 | valid ppl     2.80 | training loss  0.93 | training ppl     2.53 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  12 |   100/  562 batches | lr 20.0000 | ms/batch 117.34 | loss  1.45 | ppl     4.26 |\n",
      "| epoch  12 |   200/  562 batches | lr 20.0000 | ms/batch 116.52 | loss  1.45 | ppl     4.26 |\n",
      "| epoch  12 |   300/  562 batches | lr 20.0000 | ms/batch 116.32 | loss  1.50 | ppl     4.50 |\n",
      "| epoch  12 |   400/  562 batches | lr 20.0000 | ms/batch 115.82 | loss  1.44 | ppl     4.22 |\n",
      "| epoch  12 |   500/  562 batches | lr 20.0000 | ms/batch 115.87 | loss  1.39 | ppl     4.00 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 92.10s | valid loss  0.95 | valid ppl     2.59 | training loss  0.85 | training ppl     2.33 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  13 |   100/  562 batches | lr 20.0000 | ms/batch 117.03 | loss  1.37 | ppl     3.95 |\n",
      "| epoch  13 |   200/  562 batches | lr 20.0000 | ms/batch 115.81 | loss  1.37 | ppl     3.94 |\n",
      "| epoch  13 |   300/  562 batches | lr 20.0000 | ms/batch 115.89 | loss  1.42 | ppl     4.15 |\n",
      "| epoch  13 |   400/  562 batches | lr 20.0000 | ms/batch 115.99 | loss  1.36 | ppl     3.91 |\n",
      "| epoch  13 |   500/  562 batches | lr 20.0000 | ms/batch 115.90 | loss  1.32 | ppl     3.75 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 91.34s | valid loss  0.89 | valid ppl     2.43 | training loss  0.79 | training ppl     2.20 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  14 |   100/  562 batches | lr 20.0000 | ms/batch 115.66 | loss  1.31 | ppl     3.71 |\n",
      "| epoch  14 |   200/  562 batches | lr 20.0000 | ms/batch 114.39 | loss  1.31 | ppl     3.72 |\n",
      "| epoch  14 |   300/  562 batches | lr 20.0000 | ms/batch 114.52 | loss  1.37 | ppl     3.92 |\n",
      "| epoch  14 |   400/  562 batches | lr 20.0000 | ms/batch 114.44 | loss  1.31 | ppl     3.72 |\n",
      "| epoch  14 |   500/  562 batches | lr 20.0000 | ms/batch 114.29 | loss  1.27 | ppl     3.55 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 90.41s | valid loss  0.85 | valid ppl     2.35 | training loss  0.73 | training ppl     2.07 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  15 |   100/  562 batches | lr 20.0000 | ms/batch 115.80 | loss  1.25 | ppl     3.50 |\n",
      "| epoch  15 |   200/  562 batches | lr 20.0000 | ms/batch 114.74 | loss  1.26 | ppl     3.52 |\n",
      "| epoch  15 |   300/  562 batches | lr 20.0000 | ms/batch 114.72 | loss  1.32 | ppl     3.73 |\n",
      "| epoch  15 |   400/  562 batches | lr 20.0000 | ms/batch 114.74 | loss  1.26 | ppl     3.54 |\n",
      "| epoch  15 |   500/  562 batches | lr 20.0000 | ms/batch 114.75 | loss  1.22 | ppl     3.38 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 90.59s | valid loss  0.81 | valid ppl     2.26 | training loss  0.67 | training ppl     1.96 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  16 |   100/  562 batches | lr 20.0000 | ms/batch 116.20 | loss  1.21 | ppl     3.35 |\n",
      "| epoch  16 |   200/  562 batches | lr 20.0000 | ms/batch 114.82 | loss  1.22 | ppl     3.37 |\n",
      "| epoch  16 |   300/  562 batches | lr 20.0000 | ms/batch 114.70 | loss  1.27 | ppl     3.57 |\n",
      "| epoch  16 |   400/  562 batches | lr 20.0000 | ms/batch 115.08 | loss  1.22 | ppl     3.39 |\n",
      "| epoch  16 |   500/  562 batches | lr 20.0000 | ms/batch 115.13 | loss  1.18 | ppl     3.24 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 90.75s | valid loss  0.79 | valid ppl     2.20 | training loss  0.64 | training ppl     1.90 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  17 |   100/  562 batches | lr 20.0000 | ms/batch 115.93 | loss  1.17 | ppl     3.23 |\n",
      "| epoch  17 |   200/  562 batches | lr 20.0000 | ms/batch 114.57 | loss  1.18 | ppl     3.27 |\n",
      "| epoch  17 |   300/  562 batches | lr 20.0000 | ms/batch 114.55 | loss  1.23 | ppl     3.42 |\n",
      "| epoch  17 |   400/  562 batches | lr 20.0000 | ms/batch 114.63 | loss  1.19 | ppl     3.27 |\n",
      "| epoch  17 |   500/  562 batches | lr 20.0000 | ms/batch 114.80 | loss  1.15 | ppl     3.16 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 90.54s | valid loss  0.76 | valid ppl     2.14 | training loss  0.61 | training ppl     1.85 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  18 |   100/  562 batches | lr 20.0000 | ms/batch 116.10 | loss  1.14 | ppl     3.13 |\n",
      "| epoch  18 |   200/  562 batches | lr 20.0000 | ms/batch 115.07 | loss  1.14 | ppl     3.13 |\n",
      "| epoch  18 |   300/  562 batches | lr 20.0000 | ms/batch 114.94 | loss  1.20 | ppl     3.32 |\n",
      "| epoch  18 |   400/  562 batches | lr 20.0000 | ms/batch 115.11 | loss  1.15 | ppl     3.17 |\n",
      "| epoch  18 |   500/  562 batches | lr 20.0000 | ms/batch 114.79 | loss  1.11 | ppl     3.05 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 90.69s | valid loss  0.74 | valid ppl     2.09 | training loss  0.57 | training ppl     1.78 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  19 |   100/  562 batches | lr 20.0000 | ms/batch 116.10 | loss  1.11 | ppl     3.03 |\n",
      "| epoch  19 |   200/  562 batches | lr 20.0000 | ms/batch 115.02 | loss  1.12 | ppl     3.06 |\n",
      "| epoch  19 |   300/  562 batches | lr 20.0000 | ms/batch 115.02 | loss  1.17 | ppl     3.21 |\n",
      "| epoch  19 |   400/  562 batches | lr 20.0000 | ms/batch 114.86 | loss  1.13 | ppl     3.08 |\n",
      "| epoch  19 |   500/  562 batches | lr 20.0000 | ms/batch 114.81 | loss  1.09 | ppl     2.97 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 90.61s | valid loss  0.71 | valid ppl     2.03 | training loss  0.55 | training ppl     1.73 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  20 |   100/  562 batches | lr 20.0000 | ms/batch 116.25 | loss  1.09 | ppl     2.98 |\n",
      "| epoch  20 |   200/  562 batches | lr 20.0000 | ms/batch 114.72 | loss  1.10 | ppl     2.99 |\n",
      "| epoch  20 |   300/  562 batches | lr 20.0000 | ms/batch 114.84 | loss  1.14 | ppl     3.13 |\n",
      "| epoch  20 |   400/  562 batches | lr 20.0000 | ms/batch 115.09 | loss  1.11 | ppl     3.02 |\n",
      "| epoch  20 |   500/  562 batches | lr 20.0000 | ms/batch 114.99 | loss  1.08 | ppl     2.93 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 90.66s | valid loss  0.69 | valid ppl     2.00 | training loss  0.53 | training ppl     1.71 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  21 |   100/  562 batches | lr 20.0000 | ms/batch 116.06 | loss  1.07 | ppl     2.91 |\n",
      "| epoch  21 |   200/  562 batches | lr 20.0000 | ms/batch 114.78 | loss  1.07 | ppl     2.92 |\n",
      "| epoch  21 |   300/  562 batches | lr 20.0000 | ms/batch 114.76 | loss  1.12 | ppl     3.07 |\n",
      "| epoch  21 |   400/  562 batches | lr 20.0000 | ms/batch 114.80 | loss  1.08 | ppl     2.95 |\n",
      "| epoch  21 |   500/  562 batches | lr 20.0000 | ms/batch 114.83 | loss  1.05 | ppl     2.86 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 90.54s | valid loss  0.69 | valid ppl     1.99 | training loss  0.51 | training ppl     1.66 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  22 |   100/  562 batches | lr 20.0000 | ms/batch 116.15 | loss  1.04 | ppl     2.84 |\n",
      "| epoch  22 |   200/  562 batches | lr 20.0000 | ms/batch 114.98 | loss  1.06 | ppl     2.88 |\n",
      "| epoch  22 |   300/  562 batches | lr 20.0000 | ms/batch 114.95 | loss  1.11 | ppl     3.04 |\n",
      "| epoch  22 |   400/  562 batches | lr 20.0000 | ms/batch 115.06 | loss  1.07 | ppl     2.91 |\n",
      "| epoch  22 |   500/  562 batches | lr 20.0000 | ms/batch 115.13 | loss  1.03 | ppl     2.81 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 90.69s | valid loss  0.67 | valid ppl     1.96 | training loss  0.49 | training ppl     1.64 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  23 |   100/  562 batches | lr 20.0000 | ms/batch 115.65 | loss  1.03 | ppl     2.80 |\n",
      "| epoch  23 |   200/  562 batches | lr 20.0000 | ms/batch 114.31 | loss  1.04 | ppl     2.84 |\n",
      "| epoch  23 |   300/  562 batches | lr 20.0000 | ms/batch 114.69 | loss  1.09 | ppl     2.97 |\n",
      "| epoch  23 |   400/  562 batches | lr 20.0000 | ms/batch 114.41 | loss  1.04 | ppl     2.84 |\n",
      "| epoch  23 |   500/  562 batches | lr 20.0000 | ms/batch 114.32 | loss  1.02 | ppl     2.76 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 90.34s | valid loss  0.66 | valid ppl     1.93 | training loss  0.47 | training ppl     1.61 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  24 |   100/  562 batches | lr 20.0000 | ms/batch 115.69 | loss  1.02 | ppl     2.77 |\n",
      "| epoch  24 |   200/  562 batches | lr 20.0000 | ms/batch 114.56 | loss  1.03 | ppl     2.79 |\n",
      "| epoch  24 |   300/  562 batches | lr 20.0000 | ms/batch 114.55 | loss  1.07 | ppl     2.92 |\n",
      "| epoch  24 |   400/  562 batches | lr 20.0000 | ms/batch 114.22 | loss  1.03 | ppl     2.81 |\n",
      "| epoch  24 |   500/  562 batches | lr 20.0000 | ms/batch 114.37 | loss  1.00 | ppl     2.72 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 90.36s | valid loss  0.64 | valid ppl     1.90 | training loss  0.46 | training ppl     1.59 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  25 |   100/  562 batches | lr 20.0000 | ms/batch 115.73 | loss  1.00 | ppl     2.72 |\n",
      "| epoch  25 |   200/  562 batches | lr 20.0000 | ms/batch 114.40 | loss  1.01 | ppl     2.74 |\n",
      "| epoch  25 |   300/  562 batches | lr 20.0000 | ms/batch 114.48 | loss  1.05 | ppl     2.87 |\n",
      "| epoch  25 |   400/  562 batches | lr 20.0000 | ms/batch 114.41 | loss  1.02 | ppl     2.78 |\n",
      "| epoch  25 |   500/  562 batches | lr 20.0000 | ms/batch 114.56 | loss  0.99 | ppl     2.68 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 90.44s | valid loss  0.64 | valid ppl     1.89 | training loss  0.45 | training ppl     1.58 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  26 |   100/  562 batches | lr 20.0000 | ms/batch 116.03 | loss  0.99 | ppl     2.69 |\n",
      "| epoch  26 |   200/  562 batches | lr 20.0000 | ms/batch 114.82 | loss  1.00 | ppl     2.71 |\n",
      "| epoch  26 |   300/  562 batches | lr 20.0000 | ms/batch 114.79 | loss  1.05 | ppl     2.85 |\n",
      "| epoch  26 |   400/  562 batches | lr 20.0000 | ms/batch 114.89 | loss  1.01 | ppl     2.74 |\n",
      "| epoch  26 |   500/  562 batches | lr 20.0000 | ms/batch 114.52 | loss  0.98 | ppl     2.67 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 90.57s | valid loss  0.63 | valid ppl     1.88 | training loss  0.44 | training ppl     1.55 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  27 |   100/  562 batches | lr 20.0000 | ms/batch 115.92 | loss  0.98 | ppl     2.65 |\n",
      "| epoch  27 |   200/  562 batches | lr 20.0000 | ms/batch 114.84 | loss  0.99 | ppl     2.70 |\n",
      "| epoch  27 |   300/  562 batches | lr 20.0000 | ms/batch 114.84 | loss  1.04 | ppl     2.82 |\n",
      "| epoch  27 |   400/  562 batches | lr 20.0000 | ms/batch 114.79 | loss  1.00 | ppl     2.71 |\n",
      "| epoch  27 |   500/  562 batches | lr 20.0000 | ms/batch 114.77 | loss  0.97 | ppl     2.64 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 90.61s | valid loss  0.63 | valid ppl     1.87 | training loss  0.43 | training ppl     1.53 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  28 |   100/  562 batches | lr 20.0000 | ms/batch 116.29 | loss  0.97 | ppl     2.63 |\n",
      "| epoch  28 |   200/  562 batches | lr 20.0000 | ms/batch 115.03 | loss  0.98 | ppl     2.67 |\n",
      "| epoch  28 |   300/  562 batches | lr 20.0000 | ms/batch 115.20 | loss  1.03 | ppl     2.81 |\n",
      "| epoch  28 |   400/  562 batches | lr 20.0000 | ms/batch 115.05 | loss  0.99 | ppl     2.68 |\n",
      "| epoch  28 |   500/  562 batches | lr 20.0000 | ms/batch 115.08 | loss  0.96 | ppl     2.62 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 90.77s | valid loss  0.63 | valid ppl     1.87 | training loss  0.42 | training ppl     1.53 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  29 |   100/  562 batches | lr 13.3333 | ms/batch 116.34 | loss  0.94 | ppl     2.56 |\n",
      "| epoch  29 |   200/  562 batches | lr 13.3333 | ms/batch 115.11 | loss  0.92 | ppl     2.51 |\n",
      "| epoch  29 |   300/  562 batches | lr 13.3333 | ms/batch 115.23 | loss  0.94 | ppl     2.55 |\n",
      "| epoch  29 |   400/  562 batches | lr 13.3333 | ms/batch 115.15 | loss  0.88 | ppl     2.42 |\n",
      "| epoch  29 |   500/  562 batches | lr 13.3333 | ms/batch 115.11 | loss  0.84 | ppl     2.32 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 90.72s | valid loss  0.57 | valid ppl     1.77 | training loss  0.35 | training ppl     1.42 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  30 |   100/  562 batches | lr 13.3333 | ms/batch 116.03 | loss  0.86 | ppl     2.36 |\n",
      "| epoch  30 |   200/  562 batches | lr 13.3333 | ms/batch 114.92 | loss  0.85 | ppl     2.35 |\n",
      "| epoch  30 |   300/  562 batches | lr 13.3333 | ms/batch 114.93 | loss  0.88 | ppl     2.42 |\n",
      "| epoch  30 |   400/  562 batches | lr 13.3333 | ms/batch 114.87 | loss  0.84 | ppl     2.32 |\n",
      "| epoch  30 |   500/  562 batches | lr 13.3333 | ms/batch 115.07 | loss  0.81 | ppl     2.25 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 90.67s | valid loss  0.56 | valid ppl     1.75 | training loss  0.33 | training ppl     1.39 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  31 |   100/  562 batches | lr 13.3333 | ms/batch 115.98 | loss  0.82 | ppl     2.28 |\n",
      "| epoch  31 |   200/  562 batches | lr 13.3333 | ms/batch 114.71 | loss  0.82 | ppl     2.28 |\n",
      "| epoch  31 |   300/  562 batches | lr 13.3333 | ms/batch 114.63 | loss  0.86 | ppl     2.37 |\n",
      "| epoch  31 |   400/  562 batches | lr 13.3333 | ms/batch 114.69 | loss  0.83 | ppl     2.28 |\n",
      "| epoch  31 |   500/  562 batches | lr 13.3333 | ms/batch 114.81 | loss  0.79 | ppl     2.21 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 90.51s | valid loss  0.55 | valid ppl     1.73 | training loss  0.32 | training ppl     1.37 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  32 |   100/  562 batches | lr 13.3333 | ms/batch 116.07 | loss  0.80 | ppl     2.23 |\n",
      "| epoch  32 |   200/  562 batches | lr 13.3333 | ms/batch 114.84 | loss  0.81 | ppl     2.26 |\n",
      "| epoch  32 |   300/  562 batches | lr 13.3333 | ms/batch 114.93 | loss  0.84 | ppl     2.31 |\n",
      "| epoch  32 |   400/  562 batches | lr 13.3333 | ms/batch 114.86 | loss  0.80 | ppl     2.23 |\n",
      "| epoch  32 |   500/  562 batches | lr 13.3333 | ms/batch 114.85 | loss  0.77 | ppl     2.16 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 90.60s | valid loss  0.54 | valid ppl     1.72 | training loss  0.30 | training ppl     1.35 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  33 |   100/  562 batches | lr 13.3333 | ms/batch 115.66 | loss  0.79 | ppl     2.21 |\n",
      "| epoch  33 |   200/  562 batches | lr 13.3333 | ms/batch 114.53 | loss  0.79 | ppl     2.20 |\n",
      "| epoch  33 |   300/  562 batches | lr 13.3333 | ms/batch 114.36 | loss  0.82 | ppl     2.28 |\n",
      "| epoch  33 |   400/  562 batches | lr 13.3333 | ms/batch 114.29 | loss  0.78 | ppl     2.19 |\n",
      "| epoch  33 |   500/  562 batches | lr 13.3333 | ms/batch 114.95 | loss  0.76 | ppl     2.13 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 90.46s | valid loss  0.54 | valid ppl     1.71 | training loss  0.30 | training ppl     1.34 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  34 |   100/  562 batches | lr 13.3333 | ms/batch 115.95 | loss  0.77 | ppl     2.16 |\n",
      "| epoch  34 |   200/  562 batches | lr 13.3333 | ms/batch 114.50 | loss  0.78 | ppl     2.19 |\n",
      "| epoch  34 |   300/  562 batches | lr 13.3333 | ms/batch 114.74 | loss  0.81 | ppl     2.25 |\n",
      "| epoch  34 |   400/  562 batches | lr 13.3333 | ms/batch 114.42 | loss  0.78 | ppl     2.19 |\n",
      "| epoch  34 |   500/  562 batches | lr 13.3333 | ms/batch 114.70 | loss  0.75 | ppl     2.12 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 90.68s | valid loss  0.53 | valid ppl     1.71 | training loss  0.28 | training ppl     1.33 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  35 |   100/  562 batches | lr 13.3333 | ms/batch 116.84 | loss  0.77 | ppl     2.16 |\n",
      "| epoch  35 |   200/  562 batches | lr 13.3333 | ms/batch 116.02 | loss  0.76 | ppl     2.15 |\n",
      "| epoch  35 |   300/  562 batches | lr 13.3333 | ms/batch 116.27 | loss  0.80 | ppl     2.22 |\n",
      "| epoch  35 |   400/  562 batches | lr 13.3333 | ms/batch 116.23 | loss  0.76 | ppl     2.14 |\n",
      "| epoch  35 |   500/  562 batches | lr 13.3333 | ms/batch 116.34 | loss  0.75 | ppl     2.11 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 92.20s | valid loss  0.53 | valid ppl     1.69 | training loss  0.28 | training ppl     1.32 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  36 |   100/  562 batches | lr 13.3333 | ms/batch 118.06 | loss  0.76 | ppl     2.13 |\n",
      "| epoch  36 |   200/  562 batches | lr 13.3333 | ms/batch 116.68 | loss  0.76 | ppl     2.13 |\n",
      "| epoch  36 |   300/  562 batches | lr 13.3333 | ms/batch 116.71 | loss  0.79 | ppl     2.21 |\n",
      "| epoch  36 |   400/  562 batches | lr 13.3333 | ms/batch 116.86 | loss  0.76 | ppl     2.14 |\n",
      "| epoch  36 |   500/  562 batches | lr 13.3333 | ms/batch 116.60 | loss  0.73 | ppl     2.08 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 91.99s | valid loss  0.52 | valid ppl     1.69 | training loss  0.27 | training ppl     1.31 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  37 |   100/  562 batches | lr 13.3333 | ms/batch 115.92 | loss  0.74 | ppl     2.10 |\n",
      "| epoch  37 |   200/  562 batches | lr 13.3333 | ms/batch 114.62 | loss  0.75 | ppl     2.12 |\n",
      "| epoch  37 |   300/  562 batches | lr 13.3333 | ms/batch 114.89 | loss  0.78 | ppl     2.17 |\n",
      "| epoch  37 |   400/  562 batches | lr 13.3333 | ms/batch 114.98 | loss  0.75 | ppl     2.11 |\n",
      "| epoch  37 |   500/  562 batches | lr 13.3333 | ms/batch 114.81 | loss  0.73 | ppl     2.07 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 90.70s | valid loss  0.52 | valid ppl     1.69 | training loss  0.26 | training ppl     1.30 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  38 |   100/  562 batches | lr 13.3333 | ms/batch 116.23 | loss  0.74 | ppl     2.09 |\n",
      "| epoch  38 |   200/  562 batches | lr 13.3333 | ms/batch 115.13 | loss  0.73 | ppl     2.08 |\n",
      "| epoch  38 |   300/  562 batches | lr 13.3333 | ms/batch 115.10 | loss  0.77 | ppl     2.16 |\n",
      "| epoch  38 |   400/  562 batches | lr 13.3333 | ms/batch 115.23 | loss  0.74 | ppl     2.10 |\n",
      "| epoch  38 |   500/  562 batches | lr 13.3333 | ms/batch 115.02 | loss  0.71 | ppl     2.04 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 90.84s | valid loss  0.52 | valid ppl     1.68 | training loss  0.26 | training ppl     1.29 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  39 |   100/  562 batches | lr 13.3333 | ms/batch 116.11 | loss  0.73 | ppl     2.07 |\n",
      "| epoch  39 |   200/  562 batches | lr 13.3333 | ms/batch 115.08 | loss  0.73 | ppl     2.08 |\n",
      "| epoch  39 |   300/  562 batches | lr 13.3333 | ms/batch 115.13 | loss  0.76 | ppl     2.13 |\n",
      "| epoch  39 |   400/  562 batches | lr 13.3333 | ms/batch 115.29 | loss  0.73 | ppl     2.07 |\n",
      "| epoch  39 |   500/  562 batches | lr 13.3333 | ms/batch 115.21 | loss  0.71 | ppl     2.04 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 90.87s | valid loss  0.51 | valid ppl     1.67 | training loss  0.25 | training ppl     1.29 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  40 |   100/  562 batches | lr 13.3333 | ms/batch 116.21 | loss  0.72 | ppl     2.06 |\n",
      "| epoch  40 |   200/  562 batches | lr 13.3333 | ms/batch 114.64 | loss  0.73 | ppl     2.07 |\n",
      "| epoch  40 |   300/  562 batches | lr 13.3333 | ms/batch 114.89 | loss  0.76 | ppl     2.13 |\n",
      "| epoch  40 |   400/  562 batches | lr 13.3333 | ms/batch 115.12 | loss  0.72 | ppl     2.06 |\n",
      "| epoch  40 |   500/  562 batches | lr 13.3333 | ms/batch 115.14 | loss  0.71 | ppl     2.03 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 90.72s | valid loss  0.51 | valid ppl     1.66 | training loss  0.25 | training ppl     1.28 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  41 |   100/  562 batches | lr 13.3333 | ms/batch 116.10 | loss  0.72 | ppl     2.05 |\n",
      "| epoch  41 |   200/  562 batches | lr 13.3333 | ms/batch 115.05 | loss  0.72 | ppl     2.06 |\n",
      "| epoch  41 |   300/  562 batches | lr 13.3333 | ms/batch 114.83 | loss  0.75 | ppl     2.12 |\n",
      "| epoch  41 |   400/  562 batches | lr 13.3333 | ms/batch 115.02 | loss  0.72 | ppl     2.06 |\n",
      "| epoch  41 |   500/  562 batches | lr 13.3333 | ms/batch 114.57 | loss  0.70 | ppl     2.02 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time: 90.66s | valid loss  0.51 | valid ppl     1.67 | training loss  0.24 | training ppl     1.27 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  42 |   100/  562 batches | lr 8.8889 | ms/batch 116.23 | loss  0.70 | ppl     2.00 |\n",
      "| epoch  42 |   200/  562 batches | lr 8.8889 | ms/batch 115.08 | loss  0.68 | ppl     1.97 |\n",
      "| epoch  42 |   300/  562 batches | lr 8.8889 | ms/batch 115.37 | loss  0.70 | ppl     2.01 |\n",
      "| epoch  42 |   400/  562 batches | lr 8.8889 | ms/batch 115.06 | loss  0.66 | ppl     1.93 |\n",
      "| epoch  42 |   500/  562 batches | lr 8.8889 | ms/batch 115.14 | loss  0.63 | ppl     1.87 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time: 90.80s | valid loss  0.49 | valid ppl     1.64 | training loss  0.21 | training ppl     1.24 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  43 |   100/  562 batches | lr 8.8889 | ms/batch 115.88 | loss  0.65 | ppl     1.91 |\n",
      "| epoch  43 |   200/  562 batches | lr 8.8889 | ms/batch 114.94 | loss  0.65 | ppl     1.91 |\n",
      "| epoch  43 |   300/  562 batches | lr 8.8889 | ms/batch 114.94 | loss  0.66 | ppl     1.94 |\n",
      "| epoch  43 |   400/  562 batches | lr 8.8889 | ms/batch 114.98 | loss  0.64 | ppl     1.90 |\n",
      "| epoch  43 |   500/  562 batches | lr 8.8889 | ms/batch 114.33 | loss  0.61 | ppl     1.85 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time: 90.53s | valid loss  0.49 | valid ppl     1.63 | training loss  0.20 | training ppl     1.23 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  44 |   100/  562 batches | lr 8.8889 | ms/batch 115.98 | loss  0.64 | ppl     1.89 |\n",
      "| epoch  44 |   200/  562 batches | lr 8.8889 | ms/batch 114.76 | loss  0.63 | ppl     1.88 |\n",
      "| epoch  44 |   300/  562 batches | lr 8.8889 | ms/batch 114.97 | loss  0.65 | ppl     1.92 |\n",
      "| epoch  44 |   400/  562 batches | lr 8.8889 | ms/batch 115.00 | loss  0.62 | ppl     1.86 |\n",
      "| epoch  44 |   500/  562 batches | lr 8.8889 | ms/batch 114.90 | loss  0.60 | ppl     1.82 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time: 90.64s | valid loss  0.48 | valid ppl     1.62 | training loss  0.19 | training ppl     1.21 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  45 |   100/  562 batches | lr 8.8889 | ms/batch 115.98 | loss  0.61 | ppl     1.84 |\n",
      "| epoch  45 |   200/  562 batches | lr 8.8889 | ms/batch 115.00 | loss  0.61 | ppl     1.84 |\n",
      "| epoch  45 |   300/  562 batches | lr 8.8889 | ms/batch 114.69 | loss  0.64 | ppl     1.89 |\n",
      "| epoch  45 |   400/  562 batches | lr 8.8889 | ms/batch 114.76 | loss  0.61 | ppl     1.84 |\n",
      "| epoch  45 |   500/  562 batches | lr 8.8889 | ms/batch 115.05 | loss  0.59 | ppl     1.80 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time: 90.68s | valid loss  0.48 | valid ppl     1.62 | training loss  0.19 | training ppl     1.21 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  46 |   100/  562 batches | lr 8.8889 | ms/batch 116.46 | loss  0.60 | ppl     1.83 |\n",
      "| epoch  46 |   200/  562 batches | lr 8.8889 | ms/batch 115.19 | loss  0.60 | ppl     1.83 |\n",
      "| epoch  46 |   300/  562 batches | lr 8.8889 | ms/batch 115.20 | loss  0.63 | ppl     1.87 |\n",
      "| epoch  46 |   400/  562 batches | lr 8.8889 | ms/batch 115.24 | loss  0.60 | ppl     1.82 |\n",
      "| epoch  46 |   500/  562 batches | lr 8.8889 | ms/batch 115.23 | loss  0.58 | ppl     1.78 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time: 90.79s | valid loss  0.48 | valid ppl     1.61 | training loss  0.18 | training ppl     1.20 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  47 |   100/  562 batches | lr 8.8889 | ms/batch 115.32 | loss  0.60 | ppl     1.83 |\n",
      "| epoch  47 |   200/  562 batches | lr 8.8889 | ms/batch 114.80 | loss  0.60 | ppl     1.83 |\n",
      "| epoch  47 |   300/  562 batches | lr 8.8889 | ms/batch 114.80 | loss  0.62 | ppl     1.86 |\n",
      "| epoch  47 |   400/  562 batches | lr 8.8889 | ms/batch 114.96 | loss  0.59 | ppl     1.80 |\n",
      "| epoch  47 |   500/  562 batches | lr 8.8889 | ms/batch 114.83 | loss  0.57 | ppl     1.77 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time: 90.54s | valid loss  0.47 | valid ppl     1.61 | training loss  0.18 | training ppl     1.19 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  48 |   100/  562 batches | lr 8.8889 | ms/batch 116.31 | loss  0.59 | ppl     1.80 |\n",
      "| epoch  48 |   200/  562 batches | lr 8.8889 | ms/batch 115.15 | loss  0.59 | ppl     1.80 |\n",
      "| epoch  48 |   300/  562 batches | lr 8.8889 | ms/batch 115.21 | loss  0.61 | ppl     1.83 |\n",
      "| epoch  48 |   400/  562 batches | lr 8.8889 | ms/batch 115.18 | loss  0.58 | ppl     1.79 |\n",
      "| epoch  48 |   500/  562 batches | lr 8.8889 | ms/batch 114.97 | loss  0.56 | ppl     1.75 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time: 90.82s | valid loss  0.47 | valid ppl     1.60 | training loss  0.17 | training ppl     1.19 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  49 |   100/  562 batches | lr 8.8889 | ms/batch 116.40 | loss  0.58 | ppl     1.78 |\n",
      "| epoch  49 |   200/  562 batches | lr 8.8889 | ms/batch 115.12 | loss  0.58 | ppl     1.79 |\n",
      "| epoch  49 |   300/  562 batches | lr 8.8889 | ms/batch 115.19 | loss  0.60 | ppl     1.82 |\n",
      "| epoch  49 |   400/  562 batches | lr 8.8889 | ms/batch 115.12 | loss  0.58 | ppl     1.78 |\n",
      "| epoch  49 |   500/  562 batches | lr 8.8889 | ms/batch 115.12 | loss  0.55 | ppl     1.74 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time: 90.78s | valid loss  0.49 | valid ppl     1.63 | training loss  0.17 | training ppl     1.18 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  50 |   100/  562 batches | lr 5.9259 | ms/batch 116.55 | loss  0.57 | ppl     1.76 |\n",
      "| epoch  50 |   200/  562 batches | lr 5.9259 | ms/batch 115.28 | loss  0.55 | ppl     1.74 |\n",
      "| epoch  50 |   300/  562 batches | lr 5.9259 | ms/batch 115.29 | loss  0.57 | ppl     1.77 |\n",
      "| epoch  50 |   400/  562 batches | lr 5.9259 | ms/batch 114.33 | loss  0.54 | ppl     1.71 |\n",
      "| epoch  50 |   500/  562 batches | lr 5.9259 | ms/batch 114.88 | loss  0.52 | ppl     1.67 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time: 90.70s | valid loss  0.46 | valid ppl     1.59 | training loss  0.15 | training ppl     1.16 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  51 |   100/  562 batches | lr 5.9259 | ms/batch 116.09 | loss  0.54 | ppl     1.71 |\n",
      "| epoch  51 |   200/  562 batches | lr 5.9259 | ms/batch 114.86 | loss  0.53 | ppl     1.71 |\n",
      "| epoch  51 |   300/  562 batches | lr 5.9259 | ms/batch 114.79 | loss  0.55 | ppl     1.73 |\n",
      "| epoch  51 |   400/  562 batches | lr 5.9259 | ms/batch 114.88 | loss  0.52 | ppl     1.69 |\n",
      "| epoch  51 |   500/  562 batches | lr 5.9259 | ms/batch 114.85 | loss  0.51 | ppl     1.67 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  51 | time: 90.61s | valid loss  0.46 | valid ppl     1.59 | training loss  0.14 | training ppl     1.16 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  52 |   100/  562 batches | lr 5.9259 | ms/batch 116.19 | loss  0.52 | ppl     1.68 |\n",
      "| epoch  52 |   200/  562 batches | lr 5.9259 | ms/batch 115.18 | loss  0.52 | ppl     1.69 |\n",
      "| epoch  52 |   300/  562 batches | lr 5.9259 | ms/batch 115.06 | loss  0.54 | ppl     1.71 |\n",
      "| epoch  52 |   400/  562 batches | lr 5.9259 | ms/batch 115.08 | loss  0.51 | ppl     1.66 |\n",
      "| epoch  52 |   500/  562 batches | lr 5.9259 | ms/batch 115.09 | loss  0.49 | ppl     1.64 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  52 | time: 90.72s | valid loss  0.46 | valid ppl     1.59 | training loss  0.14 | training ppl     1.15 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  53 |   100/  562 batches | lr 3.9506 | ms/batch 116.16 | loss  0.51 | ppl     1.66 |\n",
      "| epoch  53 |   200/  562 batches | lr 3.9506 | ms/batch 114.83 | loss  0.50 | ppl     1.65 |\n",
      "| epoch  53 |   300/  562 batches | lr 3.9506 | ms/batch 114.74 | loss  0.52 | ppl     1.68 |\n",
      "| epoch  53 |   400/  562 batches | lr 3.9506 | ms/batch 114.84 | loss  0.49 | ppl     1.63 |\n",
      "| epoch  53 |   500/  562 batches | lr 3.9506 | ms/batch 114.71 | loss  0.47 | ppl     1.60 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  53 | time: 90.56s | valid loss  0.46 | valid ppl     1.58 | training loss  0.13 | training ppl     1.14 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  54 |   100/  562 batches | lr 3.9506 | ms/batch 115.99 | loss  0.49 | ppl     1.62 |\n",
      "| epoch  54 |   200/  562 batches | lr 3.9506 | ms/batch 114.80 | loss  0.48 | ppl     1.62 |\n",
      "| epoch  54 |   300/  562 batches | lr 3.9506 | ms/batch 114.88 | loss  0.50 | ppl     1.66 |\n",
      "| epoch  54 |   400/  562 batches | lr 3.9506 | ms/batch 114.82 | loss  0.47 | ppl     1.61 |\n",
      "| epoch  54 |   500/  562 batches | lr 3.9506 | ms/batch 114.87 | loss  0.45 | ppl     1.57 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  54 | time: 90.57s | valid loss  0.45 | valid ppl     1.58 | training loss  0.13 | training ppl     1.13 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  55 |   100/  562 batches | lr 3.9506 | ms/batch 115.98 | loss  0.48 | ppl     1.61 |\n",
      "| epoch  55 |   200/  562 batches | lr 3.9506 | ms/batch 114.86 | loss  0.48 | ppl     1.61 |\n",
      "| epoch  55 |   300/  562 batches | lr 3.9506 | ms/batch 114.72 | loss  0.49 | ppl     1.64 |\n",
      "| epoch  55 |   400/  562 batches | lr 3.9506 | ms/batch 114.84 | loss  0.47 | ppl     1.59 |\n",
      "| epoch  55 |   500/  562 batches | lr 3.9506 | ms/batch 114.70 | loss  0.45 | ppl     1.57 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  55 | time: 90.55s | valid loss  0.45 | valid ppl     1.57 | training loss  0.12 | training ppl     1.13 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  56 |   100/  562 batches | lr 3.9506 | ms/batch 116.40 | loss  0.47 | ppl     1.60 |\n",
      "| epoch  56 |   200/  562 batches | lr 3.9506 | ms/batch 115.12 | loss  0.47 | ppl     1.59 |\n",
      "| epoch  56 |   300/  562 batches | lr 3.9506 | ms/batch 115.11 | loss  0.48 | ppl     1.62 |\n",
      "| epoch  56 |   400/  562 batches | lr 3.9506 | ms/batch 115.10 | loss  0.46 | ppl     1.58 |\n",
      "| epoch  56 |   500/  562 batches | lr 3.9506 | ms/batch 115.18 | loss  0.44 | ppl     1.56 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  56 | time: 90.76s | valid loss  0.45 | valid ppl     1.57 | training loss  0.12 | training ppl     1.13 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  57 |   100/  562 batches | lr 3.9506 | ms/batch 115.93 | loss  0.46 | ppl     1.59 |\n",
      "| epoch  57 |   200/  562 batches | lr 3.9506 | ms/batch 114.62 | loss  0.46 | ppl     1.58 |\n",
      "| epoch  57 |   300/  562 batches | lr 3.9506 | ms/batch 114.07 | loss  0.47 | ppl     1.61 |\n",
      "| epoch  57 |   400/  562 batches | lr 3.9506 | ms/batch 114.70 | loss  0.45 | ppl     1.58 |\n",
      "| epoch  57 |   500/  562 batches | lr 3.9506 | ms/batch 114.88 | loss  0.43 | ppl     1.54 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  57 | time: 90.76s | valid loss  0.45 | valid ppl     1.57 | training loss  0.11 | training ppl     1.12 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  58 |   100/  562 batches | lr 3.9506 | ms/batch 117.58 | loss  0.46 | ppl     1.58 |\n",
      "| epoch  58 |   200/  562 batches | lr 3.9506 | ms/batch 116.71 | loss  0.45 | ppl     1.57 |\n",
      "| epoch  58 |   300/  562 batches | lr 3.9506 | ms/batch 116.76 | loss  0.47 | ppl     1.60 |\n",
      "| epoch  58 |   400/  562 batches | lr 3.9506 | ms/batch 117.05 | loss  0.45 | ppl     1.57 |\n",
      "| epoch  58 |   500/  562 batches | lr 3.9506 | ms/batch 116.28 | loss  0.43 | ppl     1.54 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  58 | time: 91.79s | valid loss  0.45 | valid ppl     1.57 | training loss  0.11 | training ppl     1.12 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  59 |   100/  562 batches | lr 3.9506 | ms/batch 116.31 | loss  0.45 | ppl     1.58 |\n",
      "| epoch  59 |   200/  562 batches | lr 3.9506 | ms/batch 115.13 | loss  0.45 | ppl     1.56 |\n",
      "| epoch  59 |   300/  562 batches | lr 3.9506 | ms/batch 115.14 | loss  0.47 | ppl     1.59 |\n",
      "| epoch  59 |   400/  562 batches | lr 3.9506 | ms/batch 115.04 | loss  0.45 | ppl     1.56 |\n",
      "| epoch  59 |   500/  562 batches | lr 3.9506 | ms/batch 115.17 | loss  0.42 | ppl     1.52 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  59 | time: 90.83s | valid loss  0.45 | valid ppl     1.56 | training loss  0.11 | training ppl     1.12 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  60 |   100/  562 batches | lr 3.9506 | ms/batch 116.20 | loss  0.44 | ppl     1.55 |\n",
      "| epoch  60 |   200/  562 batches | lr 3.9506 | ms/batch 115.16 | loss  0.44 | ppl     1.55 |\n",
      "| epoch  60 |   300/  562 batches | lr 3.9506 | ms/batch 115.17 | loss  0.46 | ppl     1.58 |\n",
      "| epoch  60 |   400/  562 batches | lr 3.9506 | ms/batch 114.96 | loss  0.44 | ppl     1.55 |\n",
      "| epoch  60 |   500/  562 batches | lr 3.9506 | ms/batch 114.86 | loss  0.42 | ppl     1.52 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  60 | time: 90.76s | valid loss  0.45 | valid ppl     1.56 | training loss  0.11 | training ppl     1.11 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  61 |   100/  562 batches | lr 2.6337 | ms/batch 115.87 | loss  0.44 | ppl     1.55 |\n",
      "| epoch  61 |   200/  562 batches | lr 2.6337 | ms/batch 114.97 | loss  0.43 | ppl     1.54 |\n",
      "| epoch  61 |   300/  562 batches | lr 2.6337 | ms/batch 114.68 | loss  0.45 | ppl     1.57 |\n",
      "| epoch  61 |   400/  562 batches | lr 2.6337 | ms/batch 114.94 | loss  0.43 | ppl     1.53 |\n",
      "| epoch  61 |   500/  562 batches | lr 2.6337 | ms/batch 114.90 | loss  0.40 | ppl     1.50 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  61 | time: 90.63s | valid loss  0.44 | valid ppl     1.56 | training loss  0.10 | training ppl     1.11 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  62 |   100/  562 batches | lr 2.6337 | ms/batch 116.09 | loss  0.42 | ppl     1.53 |\n",
      "| epoch  62 |   200/  562 batches | lr 2.6337 | ms/batch 114.97 | loss  0.42 | ppl     1.53 |\n",
      "| epoch  62 |   300/  562 batches | lr 2.6337 | ms/batch 115.13 | loss  0.44 | ppl     1.55 |\n",
      "| epoch  62 |   400/  562 batches | lr 2.6337 | ms/batch 114.91 | loss  0.41 | ppl     1.51 |\n",
      "| epoch  62 |   500/  562 batches | lr 2.6337 | ms/batch 114.86 | loss  0.40 | ppl     1.50 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  62 | time: 90.73s | valid loss  0.44 | valid ppl     1.56 | training loss  0.10 | training ppl     1.11 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  63 |   100/  562 batches | lr 2.6337 | ms/batch 116.07 | loss  0.42 | ppl     1.52 |\n",
      "| epoch  63 |   200/  562 batches | lr 2.6337 | ms/batch 114.87 | loss  0.41 | ppl     1.51 |\n",
      "| epoch  63 |   300/  562 batches | lr 2.6337 | ms/batch 114.94 | loss  0.43 | ppl     1.54 |\n",
      "| epoch  63 |   400/  562 batches | lr 2.6337 | ms/batch 114.81 | loss  0.41 | ppl     1.51 |\n",
      "| epoch  63 |   500/  562 batches | lr 2.6337 | ms/batch 114.88 | loss  0.39 | ppl     1.48 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  63 | time: 90.67s | valid loss  0.44 | valid ppl     1.56 | training loss  0.10 | training ppl     1.10 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  64 |   100/  562 batches | lr 1.7558 | ms/batch 115.78 | loss  0.41 | ppl     1.51 |\n",
      "| epoch  64 |   200/  562 batches | lr 1.7558 | ms/batch 114.21 | loss  0.41 | ppl     1.51 |\n",
      "| epoch  64 |   300/  562 batches | lr 1.7558 | ms/batch 114.85 | loss  0.43 | ppl     1.53 |\n",
      "| epoch  64 |   400/  562 batches | lr 1.7558 | ms/batch 114.76 | loss  0.40 | ppl     1.49 |\n",
      "| epoch  64 |   500/  562 batches | lr 1.7558 | ms/batch 114.81 | loss  0.38 | ppl     1.47 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  64 | time: 90.53s | valid loss  0.44 | valid ppl     1.56 | training loss  0.10 | training ppl     1.10 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  65 |   100/  562 batches | lr 1.7558 | ms/batch 115.90 | loss  0.40 | ppl     1.50 |\n",
      "| epoch  65 |   200/  562 batches | lr 1.7558 | ms/batch 114.96 | loss  0.40 | ppl     1.50 |\n",
      "| epoch  65 |   300/  562 batches | lr 1.7558 | ms/batch 114.91 | loss  0.42 | ppl     1.52 |\n",
      "| epoch  65 |   400/  562 batches | lr 1.7558 | ms/batch 114.93 | loss  0.39 | ppl     1.48 |\n",
      "| epoch  65 |   500/  562 batches | lr 1.7558 | ms/batch 114.96 | loss  0.38 | ppl     1.46 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  65 | time: 90.67s | valid loss  0.44 | valid ppl     1.55 | training loss  0.09 | training ppl     1.10 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  66 |   100/  562 batches | lr 1.7558 | ms/batch 116.46 | loss  0.40 | ppl     1.49 |\n",
      "| epoch  66 |   200/  562 batches | lr 1.7558 | ms/batch 115.26 | loss  0.40 | ppl     1.49 |\n",
      "| epoch  66 |   300/  562 batches | lr 1.7558 | ms/batch 115.22 | loss  0.41 | ppl     1.51 |\n",
      "| epoch  66 |   400/  562 batches | lr 1.7558 | ms/batch 115.42 | loss  0.40 | ppl     1.48 |\n",
      "| epoch  66 |   500/  562 batches | lr 1.7558 | ms/batch 115.17 | loss  0.37 | ppl     1.45 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  66 | time: 90.88s | valid loss  0.44 | valid ppl     1.56 | training loss  0.09 | training ppl     1.10 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  67 |   100/  562 batches | lr 1.1706 | ms/batch 115.96 | loss  0.39 | ppl     1.48 |\n",
      "| epoch  67 |   200/  562 batches | lr 1.1706 | ms/batch 114.99 | loss  0.40 | ppl     1.49 |\n",
      "| epoch  67 |   300/  562 batches | lr 1.1706 | ms/batch 115.03 | loss  0.41 | ppl     1.50 |\n",
      "| epoch  67 |   400/  562 batches | lr 1.1706 | ms/batch 114.80 | loss  0.39 | ppl     1.47 |\n",
      "| epoch  67 |   500/  562 batches | lr 1.1706 | ms/batch 114.10 | loss  0.37 | ppl     1.44 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  67 | time: 90.55s | valid loss  0.44 | valid ppl     1.56 | training loss  0.09 | training ppl     1.09 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  68 |   100/  562 batches | lr 0.7804 | ms/batch 116.34 | loss  0.39 | ppl     1.48 |\n",
      "| epoch  68 |   200/  562 batches | lr 0.7804 | ms/batch 115.13 | loss  0.38 | ppl     1.46 |\n",
      "| epoch  68 |   300/  562 batches | lr 0.7804 | ms/batch 115.08 | loss  0.40 | ppl     1.50 |\n",
      "| epoch  68 |   400/  562 batches | lr 0.7804 | ms/batch 115.09 | loss  0.39 | ppl     1.47 |\n",
      "| epoch  68 |   500/  562 batches | lr 0.7804 | ms/batch 115.25 | loss  0.36 | ppl     1.43 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  68 | time: 90.76s | valid loss  0.44 | valid ppl     1.56 | training loss  0.09 | training ppl     1.09 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  69 |   100/  562 batches | lr 0.5202 | ms/batch 116.29 | loss  0.39 | ppl     1.48 |\n",
      "| epoch  69 |   200/  562 batches | lr 0.5202 | ms/batch 114.96 | loss  0.38 | ppl     1.47 |\n",
      "| epoch  69 |   300/  562 batches | lr 0.5202 | ms/batch 115.12 | loss  0.40 | ppl     1.49 |\n",
      "| epoch  69 |   400/  562 batches | lr 0.5202 | ms/batch 115.16 | loss  0.38 | ppl     1.46 |\n",
      "| epoch  69 |   500/  562 batches | lr 0.5202 | ms/batch 115.28 | loss  0.36 | ppl     1.43 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  69 | time: 90.76s | valid loss  0.44 | valid ppl     1.56 | training loss  0.09 | training ppl     1.09 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| epoch  70 |   100/  562 batches | lr 0.3468 | ms/batch 116.48 | loss  0.38 | ppl     1.47 |\n",
      "| epoch  70 |   200/  562 batches | lr 0.3468 | ms/batch 115.19 | loss  0.39 | ppl     1.47 |\n",
      "| epoch  70 |   300/  562 batches | lr 0.3468 | ms/batch 115.25 | loss  0.39 | ppl     1.48 |\n",
      "| epoch  70 |   400/  562 batches | lr 0.3468 | ms/batch 115.09 | loss  0.38 | ppl     1.46 |\n",
      "| epoch  70 |   500/  562 batches | lr 0.3468 | ms/batch 115.14 | loss  0.35 | ppl     1.42 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "| end of epoch  70 | time: 90.82s | valid loss  0.44 | valid ppl     1.55 | training loss  0.09 | training ppl     1.09 |\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "Exiting from training early\n"
     ]
    }
   ],
   "source": [
    "# Start training the model\n",
    "\n",
    "best_val_loss = None\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "try:\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Start training for one epoch\n",
    "        train()\n",
    "        \n",
    "        # Get and store validation and training losses \n",
    "        val_loss = evaluate(val_data, eval_batch_size)\n",
    "        tr_loss = evaluate(train_data, batch_size)\n",
    "        \n",
    "        \n",
    "        training_loss.append(tr_loss)\n",
    "        validation_loss.append(val_loss)\n",
    "        \n",
    "        print('-' * 122)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f} | training loss {:5.2f} | training ppl {:8.2f} |'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss), tr_loss, math.exp(tr_loss)))\n",
    "        print('-' * 122)\n",
    "        \n",
    "        \n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            torch.save(model.state_dict(), \"model.pt\")\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "            learning_rate = learning_rate /1.5\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 122)\n",
    "    print('Exiting from training early')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G_Bdgv1SiIg9"
   },
   "source": [
    "# Plotting the performance graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 633
    },
    "colab_type": "code",
    "id": "6RP8p_uxFmRp",
    "outputId": "01d7e4ea-8320-40f0-9494-8b09e4bbe060"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAs8AAAJoCAYAAACZawzhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIg0kcKziIiIiEgTKTyLiIiIiDTR/wM5Q7IEd8WJywAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc3e5fab588>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's plot the validation and training loss in different epochs\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.set_style(\"ticks\")\n",
    "sns.set_context(\"poster\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs Epochs\", fontsize=30)\n",
    "\n",
    "\n",
    "# ax.text(0.5, 0.9, 'Epochs: {}'.format(num_epochs),\n",
    "#         verticalalignment='bottom', horizontalalignment='left',\n",
    "#         transform=ax.transAxes,\n",
    "#         color='Red', fontsize=20)\n",
    "\n",
    "ax.text(0.5, 0.9, 'Layers: {}'.format(num_layers),\n",
    "        verticalalignment='bottom', horizontalalignment='left',\n",
    "        transform=ax.transAxes,\n",
    "        color='Red', fontsize=20)\n",
    "\n",
    "\n",
    "ax.text(0.5, 0.85, 'Hidden size: {}'.format(hidden_size),\n",
    "        verticalalignment='bottom', horizontalalignment='left',\n",
    "        transform=ax.transAxes,\n",
    "        color='Red', fontsize=20)\n",
    "\n",
    "\n",
    "ax.text(0.5, 0.80, 'Batch Size: {}'.format(batch_size),\n",
    "        verticalalignment='bottom', horizontalalignment='left',\n",
    "        transform=ax.transAxes,\n",
    "        color='Red', fontsize=20)\n",
    "\n",
    "ax.text(0.5, 0.75, 'Seq length: {}'.format(seq_length),\n",
    "        verticalalignment='bottom', horizontalalignment='left',\n",
    "        transform=ax.transAxes,\n",
    "        color='Red', fontsize=20)\n",
    "\n",
    "# ax.text(0.5, 0.65, 'Learning rate: {:5.4f}'.format(learning_rate),\n",
    "#         verticalalignment='bottom', horizontalalignment='left',\n",
    "#         transform=ax.transAxes,\n",
    "#         color='Red', fontsize=20)\n",
    "\n",
    "ax.text(0.5, 0.70, 'Embedding size: {}'.format(embed_size),\n",
    "        verticalalignment='bottom', horizontalalignment='left',\n",
    "        transform=ax.transAxes,\n",
    "        color='Red', fontsize=20)\n",
    "\n",
    "ax.text(0.5, 0.65, 'Dropout: {}'.format(dropout_value),\n",
    "        verticalalignment='bottom', horizontalalignment='left',\n",
    "        transform=ax.transAxes,\n",
    "        color='Red', fontsize=20)\n",
    "\n",
    "\n",
    "plt.plot( training_loss, color='black', linewidth = 2, label='Training Loss')\n",
    "plt.plot( validation_loss, color='blue', linewidth = 2, label='Validation Loss')\n",
    "plt.legend(loc='best')\n",
    "fig.set_size_inches(12, 10)\n",
    "plt.savefig(\"loss_iter_graph.png\", dpi=600)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lmRlNQk0F7JA"
   },
   "outputs": [],
   "source": [
    "files.download('loss_iter_graph.png')\n",
    "\n",
    "# files.download('model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vUyLCS4-iQSJ"
   },
   "source": [
    "# Upload the trained model to Google Drive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ftKW8qkdv1pu"
   },
   "outputs": [],
   "source": [
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KzICjU1ywBcr"
   },
   "outputs": [],
   "source": [
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9Quz_8gmwPV-",
    "outputId": "8d210647-5fcb-42f3-fc11-1a7e04eae62b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file with ID 1HRK6l6WPjba3tmhdDsv752pfOB2aQ9T2\n"
     ]
    }
   ],
   "source": [
    "uploaded = drive.CreateFile({'trainedmodel': 'model.pt'})\n",
    "uploaded.SetContentFile('model.pt')\n",
    "uploaded.Upload()\n",
    "print('Uploaded file with ID {}'.format(uploaded.get('id')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "95ArpL5ZBqAg"
   },
   "source": [
    "# Evaluation and Sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "GX5f-aFGAHU8",
    "outputId": "3768bc1a-63a0-4d40-e865-6e4c92c60070"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Redirecting output to ‘wget-log.1’.\n"
     ]
    }
   ],
   "source": [
    "# files.download('model.pt')\n",
    "# You can download the model directly from Colab using the above command. But sometimes, Colab behaves weird and corrupt your file while downloading. So I chose to upload it to Google Drive \n",
    "# first, then download it to a local machine. \n",
    "\n",
    "# This trained model is downloaded hosted in my GitHub repo. I you're training your own, then you may need to host it up somewhere else and get it for testing. \n",
    "# You can directly upload the model to Colab, but trust me, uploading is boring in Colab. \n",
    "\n",
    "!wget https://github.com/sleebapaul/gospel_of_rnn/raw/master/best_model/model.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-N3ykBGL3YRm"
   },
   "outputs": [],
   "source": [
    "# Define model and load the states\n",
    "# Remember, it won't work if you've changed the hyperparameters\n",
    "\n",
    "model = RNNModel(ntoken=vocab_size, ninp=embed_size, nhid=hidden_size, nlayers=num_layers, dropout=dropout_value).to(device)\n",
    "model.load_state_dict(torch.load(\"model.pt\"))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "0xGJ70--Ftqf",
    "outputId": "a5f491b7-38e5-4226-b62a-933a4b96ba53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  0.11 | test ppl     1.11\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run on test data.\n",
    "test_data = batchify(tensor_generator(corpora[\"matthew_ylt.txt\"],word2idx), eval_batch_size)\n",
    "test_loss = evaluate(test_data, eval_batch_size)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3usK0UjsFwz8"
   },
   "source": [
    "### If you would like to generate a number of words and save it to a file, you may use the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "id": "wCShU5Oyr8jQ",
    "outputId": "96117aa3-d6a5-4306-924f-754c71c5c576"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Sampled [1000/25000] words and save to sample.txt|\n",
      "|Sampled [2000/25000] words and save to sample.txt|\n",
      "|Sampled [3000/25000] words and save to sample.txt|\n",
      "|Sampled [4000/25000] words and save to sample.txt|\n",
      "|Sampled [5000/25000] words and save to sample.txt|\n",
      "|Sampled [6000/25000] words and save to sample.txt|\n",
      "|Sampled [7000/25000] words and save to sample.txt|\n",
      "|Sampled [8000/25000] words and save to sample.txt|\n",
      "|Sampled [9000/25000] words and save to sample.txt|\n",
      "|Sampled [10000/25000] words and save to sample.txt|\n",
      "|Sampled [11000/25000] words and save to sample.txt|\n",
      "|Sampled [12000/25000] words and save to sample.txt|\n",
      "|Sampled [13000/25000] words and save to sample.txt|\n",
      "|Sampled [14000/25000] words and save to sample.txt|\n",
      "|Sampled [15000/25000] words and save to sample.txt|\n",
      "|Sampled [16000/25000] words and save to sample.txt|\n",
      "|Sampled [17000/25000] words and save to sample.txt|\n",
      "|Sampled [18000/25000] words and save to sample.txt|\n",
      "|Sampled [19000/25000] words and save to sample.txt|\n",
      "|Sampled [20000/25000] words and save to sample.txt|\n",
      "|Sampled [21000/25000] words and save to sample.txt|\n",
      "|Sampled [22000/25000] words and save to sample.txt|\n",
      "|Sampled [23000/25000] words and save to sample.txt|\n",
      "|Sampled [24000/25000] words and save to sample.txt|\n",
      "|Sampled [25000/25000] words and save to sample.txt|\n"
     ]
    }
   ],
   "source": [
    "num_samples = 25000   # Number of words to be generated \n",
    "temperature = 0.8     # Adjust temperature from 0.0 to 1.0 for diversity\n",
    "\n",
    "chapter = 1\n",
    "verses = 1\n",
    "model.eval()         \n",
    "\n",
    "# No warmup \n",
    "hidden = model.init_hidden(1)\n",
    "\n",
    "# initial_input = torch.LongTensor([[word2idx[\"<SOC>\"]]])\n",
    "initial_input = torch.randint(vocab_size, (1, 1), dtype=torch.long).to(device)\n",
    "input = initial_input.to(device)\n",
    "\n",
    "with open('generated_sample.txt', 'w') as outf:\n",
    "    \n",
    "    outf.write(\"\\n\\n\")\n",
    "    with torch.no_grad():  # no tracking history\n",
    "        for i in range(num_samples):\n",
    "            \n",
    "            output, hidden = model(input, hidden)\n",
    "            word_weights = output.squeeze().div(temperature).exp().cpu()\n",
    "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "\n",
    "            word = idx2word[word_idx.item()]\n",
    "            \n",
    "            # Maximum verses in limited to 60 by EDA\n",
    "            if verses >= 60 and word != \"<EOC>\":\n",
    "                word_idx = torch.tensor(word2idx[\"<SOC>\"])\n",
    "                word = idx2word[word_idx.item()]\n",
    "\n",
    "\n",
    "            if word == \"<EOC>\":\n",
    "                word = \"\\n\\n\"\n",
    "                \n",
    "            elif word == \"<SOC>\":\n",
    "                word = \"\\n\\n\" + str(chapter) + \"\\n\\n\"\n",
    "                chapter += 1\n",
    "                verses = 1\n",
    "\n",
    "            elif word == \"<SOV>\":\n",
    "                word = \"\\n\" + str(verses) + \" \"\n",
    "                verses += 1\n",
    "            elif word == \"<EOV>\":\n",
    "                word = \"\\n\"\n",
    "            else:\n",
    "                word =  word + ' '\n",
    "\n",
    "            outf.write(word)\n",
    "            if (i+1) % 1000 == 0:\n",
    "                print('|Sampled [{}/{}] words and save to {}|'.format(i+1, num_samples, 'sample.txt'))   \n",
    "            \n",
    "            input.fill_(word_idx)\n",
    "            \n",
    "            if chapter>28:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-bj07nkHkWXA"
   },
   "outputs": [],
   "source": [
    "files.download('generated_sample.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kHCMWKOckXX8"
   },
   "source": [
    "#### If you would like to generate with a warmup primer text, use the below function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OabrrLVeIYJj"
   },
   "outputs": [],
   "source": [
    "def generate_chapter(model, context, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Generate a chapter with given context and model\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    word = \"<SOC>\"    # Starting token\n",
    "    \n",
    "    initial_input = torch.LongTensor([[word2idx[word]]])\n",
    "    input = initial_input.to(device)\n",
    "    \n",
    "    generated_text = \"\\n1 \"\n",
    "    verses = 2\n",
    "    \n",
    "    with torch.no_grad():  # no tracking history\n",
    "        \n",
    "        while word != \"<EOC>\": \n",
    "            output, context = model(input, context)\n",
    "            word_weights = output.squeeze().div(temperature).exp().cpu()\n",
    "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "\n",
    "            word = idx2word[word_idx.item()]\n",
    "            \n",
    "            if verses > 59 and word != \"<EOC>\":\n",
    "                break\n",
    "                \n",
    "            if word == \"<SOC>\":\n",
    "                word = \"\\n\\n\"\n",
    "                break\n",
    "\n",
    "            if word == \"<SOV>\":\n",
    "                word = \"\\n\" + str(verses) + \" \"\n",
    "                verses += 1\n",
    "            elif word == \"<EOV>\":\n",
    "                word = \"\"\n",
    "            else:\n",
    "                word =  word + ' '\n",
    "\n",
    "            generated_text += word\n",
    "            word = idx2word[word_idx.item()]\n",
    "            input.fill_(word_idx)\n",
    "    \n",
    "    generated_text = generated_text.strip()\n",
    "    generated_text = generated_text.rstrip(\"<EOC>\")\n",
    "    generated_text = generated_text.rstrip(\"59\")\n",
    "    \n",
    "    generated_text = generated_text.strip()\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "TTT4doDrN1DI",
    "outputId": "175e60dc-f11b-4bbf-bf97-90ac6720af60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th chapter is generated\n",
      "2th chapter is generated\n",
      "3th chapter is generated\n",
      "4th chapter is generated\n",
      "5th chapter is generated\n",
      "6th chapter is generated\n",
      "7th chapter is generated\n",
      "8th chapter is generated\n",
      "9th chapter is generated\n",
      "10th chapter is generated\n",
      "11th chapter is generated\n",
      "12th chapter is generated\n",
      "13th chapter is generated\n",
      "14th chapter is generated\n",
      "15th chapter is generated\n",
      "16th chapter is generated\n"
     ]
    }
   ],
   "source": [
    "# Generate the Gospel of LSTM with your context\n",
    "\n",
    "gospel_of_lstms = \"1\\n\\n\"\n",
    "context = re.split(\" <SOC>\", corpora[\"mark_asv.txt\"])\n",
    "for i, text in enumerate(context):\n",
    "    if not text.startswith(\"<SOC>\"):\n",
    "        text = \"<SOC> \" + text\n",
    "    text = text.strip()\n",
    "    text = batchify(tensor_generator(text, word2idx), 1)\n",
    "    warmup_state = get_warmup_state(text)\n",
    "#     warmup_state = model.init_hidden(1)\n",
    "    \n",
    "    output = generate_chapter(model, warmup_state, temperature= 0.5)\n",
    "    gospel_of_lstms += output + \"\\n\\n\" + str(i+2) + \"\\n\\n\"\n",
    "    \n",
    "    print(\"{}th chapter is generated\".format(i+1))\n",
    "\n",
    "gospel_of_lstms = gospel_of_lstms.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9105WZB9p7HD"
   },
   "outputs": [],
   "source": [
    "with open(\"gospel_of_lstms.txt\", \"w\") as text_file:\n",
    "    text_file.write(gospel_of_lstms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IVhANdJOuHdw"
   },
   "outputs": [],
   "source": [
    "files.download('gospel_of_lstms.txt')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pytorch_LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
