{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file_name):\n",
    "    \"\"\"\n",
    "    Load the json file to a json object\n",
    "    \"\"\"\n",
    "    return json.load(open(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the complete list of vocab by adding all the individual vocabularies\n",
    "\n",
    "matthew_vocab = load_json('Matthew_vocab.json')\n",
    "mark_vocab = load_json('Mark_vocab.json')\n",
    "luke_vocab = load_json('Luke_vocab.json')\n",
    "john_vocab = load_json('John_vocab.json')\n",
    "matthew_vocab = list(matthew_vocab.keys())\n",
    "mark_vocab = list(mark_vocab.keys())\n",
    "luke_vocab = list(luke_vocab.keys())\n",
    "john_vocab = list(john_vocab.keys())\n",
    "complete_vocab = list(set(matthew_vocab + mark_vocab + luke_vocab + john_vocab))\n",
    "complete_vocab = {\"vocab\": complete_corpus}\n",
    "with open('complete_vocab.json', 'w') as fp:\n",
    "    json.dump(complete_vocab, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the complete list of verses by adding all the individual vocabularies\n",
    "\n",
    "matthew_cleaned = load_json('data/Matthew_cleaned.json')\n",
    "mark_cleaned = load_json('data/Mark_cleaned.json')\n",
    "luke_cleaned = load_json('data/Luke_cleaned.json')\n",
    "john_cleaned = load_json('data/John_cleaned.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "matthew_cleaned = [item for sublist in matthew_cleaned.values() for item in sublist]\n",
    "mark_cleaned = [item for sublist in mark_cleaned.values() for item in sublist]\n",
    "luke_cleaned = [item for sublist in luke_cleaned.values() for item in sublist]\n",
    "john_cleaned = [item for sublist in john_cleaned.values() for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_verses = {\"vocab\": john_cleaned}\n",
    "with open('data/training_john.json', 'w') as fp:\n",
    "    json.dump(complete_verses, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_big_corpus = matthew_cleaned + mark_cleaned + luke_cleaned + john_cleaned\n",
    "with open('data/single_big_corpus.json', 'w') as fp:\n",
    "    json.dump(single_big_corpus, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = load_json('data/single_big_corpus.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_corpus = []\n",
    "for line in corpus:\n",
    "    line = line.replace('<EOV>', '')\n",
    "    line = line.replace('<EOC>', '')\n",
    "    line = line.replace('<SOV>', '')\n",
    "    line = line.strip()\n",
    "    new_corpus.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_char_rnn.txt\", \"w\") as text_file:\n",
    "    text_file.write(\"\\n\".join(new_corpus))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
