{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file_name):\n",
    "    \"\"\"\n",
    "    Load the json file to a json object\n",
    "    \"\"\"\n",
    "    return json.load(open(file_name))\n",
    "\n",
    "def rearrange_data(data):\n",
    "    \"\"\"\n",
    "    Rearrange the input json schema to more \n",
    "    convenient form\n",
    "    \"\"\"\n",
    "    out_dict = {}\n",
    "    for chapter in data['chapters']:\n",
    "        chapter_num = chapter['chapter']\n",
    "        verse_list = chapter['verses'] \n",
    "        temp_list = []\n",
    "        for verse in verse_list:\n",
    "            temp_list.extend(list(verse.values()))\n",
    "        out_dict[chapter_num] = temp_list\n",
    "    return out_dict\n",
    "\n",
    "def build_vocabulary(corpus):\n",
    "    vocab_dict = {}\n",
    "    for verse in corpus:\n",
    "        doc = nlp(verse)\n",
    "        for token in doc:\n",
    "            if token.text not in vocab_dict.keys():\n",
    "                vocab_dict[token.text] = 1\n",
    "            else:\n",
    "                vocab_dict[token.text] = vocab_dict[token.text] + 1\n",
    "    return vocab_dict\n",
    "\n",
    "def create_trainable_seq(corpus):\n",
    "    \"\"\"\n",
    "    Add <SOV>, <EOV> and <EOC> tags\n",
    "    Separate each tokens by a space to use it directly for training with line.split()\n",
    "    \"\"\"\n",
    "    corpus_with_flags = {}\n",
    "    for chapter, verses in data_arranged.items():\n",
    "        temp_list = []\n",
    "        for i, verse in enumerate(verses):\n",
    "            doc = nlp(verse)\n",
    "            temp_verse = \" \"\n",
    "            for token in doc:\n",
    "                temp_verse = temp_verse + \" \" + token.text\n",
    "            temp_verse = temp_verse.strip()\n",
    "            if i != len(verses)-1:\n",
    "                new_verse = \"<SOV> \" + temp_verse + \" <EOV>\"\n",
    "            else:\n",
    "                new_verse = \"<SOV> \" + temp_verse + \" <EOV> <EOC>\"\n",
    "            temp_list.append(new_verse)\n",
    "        corpus_with_flags[chapter] = temp_list\n",
    "    return corpus_with_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file \n",
    "data = load_json('Matthew.json')\n",
    "\n",
    "# Initial arrangement\n",
    "\n",
    "data_arranged = rearrange_data(data)\n",
    "\n",
    "# Build corpus which is a list of verses from all chapters\n",
    "corpus = []\n",
    "for i, j in data_arranged.items():\n",
    "    corpus.extend(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The book of the generation of Jesus Christ, the son of David, the son of Abraham.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary dictionary with key as word and value as count\n",
    "vocab_dict = build_vocabulary(corpus)\n",
    "\n",
    "# Write it to a json file \n",
    "with open('Matthew_vocab.json', 'w') as outfile:\n",
    "    json.dump(vocab_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make trainable sequences \n",
    "sequences = create_trainable_seq(corpus)\n",
    "with open('Matthew_cleaned.json', 'w') as fp:\n",
    "    json.dump(sequences, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
